{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision flask flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from flask import Flask, request, jsonify\n",
    "from datasets import Dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "DATASET_PATH = \"C:\\\\Users\\\\91807\\\\OneDrive\\\\Desktop\\\\TOC\\\\writingPrompts\"\n",
    "MODEL_PATH = \"C:\\\\Users\\\\91807\\\\OneDrive\\\\Desktop\\\\TOC\\\\gpt2-tokenizer\"\n",
    "OUTPUT_PATH = \"C:\\\\Users\\\\91807\\\\OneDrive\\\\Desktop\\\\TOC\\\\output\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset function\n",
    "def load_dataset(source_file, target_file):\n",
    "    source_path = os.path.join(DATASET_PATH, source_file)\n",
    "    target_path = os.path.join(DATASET_PATH, target_file)\n",
    "    with open(source_path, 'r', encoding='utf-8') as src, open(target_path, 'r', encoding='utf-8') as tgt:\n",
    "        prompts = src.readlines()\n",
    "        stories = tgt.readlines()\n",
    "    return pd.DataFrame({\"prompt\": prompts, \"story\": stories})\n",
    "\n",
    "# Load and preprocess datasets\n",
    "train_df = load_dataset(\"train.wp_source\", \"train.wp_target\")\n",
    "valid_df = load_dataset(\"valid.wp_source\", \"valid.wp_target\")\n",
    "test_df = load_dataset(\"test.wp_source\", \"test.wp_target\")\n",
    "train_df.to_csv(os.path.join(OUTPUT_PATH, \"train.csv\"), index=False)\n",
    "valid_df.to_csv(os.path.join(OUTPUT_PATH, \"valid.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_PATH, \"test.csv\"), index=False)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True).train_test_split(test_size=0.1)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_PATH, \"gpt2_finetuned\"),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "model.save_pretrained(os.path.join(OUTPUT_PATH, \"gpt2_finetuned\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_PATH, \"gpt2_finetuned\"))\n",
    "\n",
    "# Save model\n",
    "model.save(os.path.join(OUTPUT_PATH, \"story_generation_model.h5\"))\n",
    "\n",
    "# Load and generate story\n",
    "def generate_story(prompt, model, tokenizer, max_seq_length=100):\n",
    "    prompt_seq = tokenizer.texts_to_sequences([prompt])\n",
    "    prompt_padded = pad_sequences(prompt_seq, maxlen=max_seq_length, padding='post')\n",
    "    decoder_input = np.zeros((1, max_seq_length))\n",
    "    decoder_input[0, 0] = tokenizer.word_index['<start>']\n",
    "    \n",
    "    for i in range(1, max_seq_length):\n",
    "        predictions = model.predict([prompt_padded, decoder_input])\n",
    "        predicted_id = np.argmax(predictions[0, i-1, :])\n",
    "        decoder_input[0, i] = predicted_id\n",
    "        if predicted_id == tokenizer.word_index['<end>']:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.sequences_to_texts(decoder_input)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.json\n",
    "    prompt = data.get('prompt')\n",
    "    story = generate_story(prompt, model, tokenizer)\n",
    "    return jsonify({\"story\": story})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
